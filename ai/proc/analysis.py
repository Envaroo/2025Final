import multiprocessing
import time
import torch
import numpy as np
import google.generativeai as genai
import json
from sentence_transformers import SentenceTransformer, util
from urllib.parse import urlparse
from pathwork import resource_path

# ==============================================================================
# 1. ì„¤ì • ë° Mock ë°ì´í„° (API í‚¤ ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •)
# ==============================================================================
USE_REAL_API = True  # Trueì¼ ê²½ìš° ì‹¤ì œ Gemini/Embedding ëª¨ë¸ ì‚¬ìš©
with open('settings.json', 'r') as f:
    data = json.load(f)
GOOGLE_API_KEY = data['APIKEY']

if USE_REAL_API:
    genai.configure(api_key=GOOGLE_API_KEY)

WHITELIST = data['WHITE']
BLACKLIST = data['BLACK']

# ==============================================================================
# 2. Worker Process Class (ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë¨)
# ==============================================================================

class FocusAnalysisProcess(multiprocessing.Process):
    def __init__(self, user_goal, task_queue, result_queue, status_event):
        """
        user_goal: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì´ˆê¸° ëª©í‘œ
        task_queue: ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì›¹í˜ì´ì§€ ì •ë³´ë¥¼ ë³´ë‚´ëŠ” í†µë¡œ
        result_queue: ë¶„ì„ ê²°ê³¼ë¥¼ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë¡œ ë³´ë‚´ëŠ” í†µë¡œ
        status_event: ì´ˆê¸°í™”(ëª¨ë¸ ë¡œë“œ/ì¿¼ë¦¬ í™•ì¥) ì™„ë£Œ ì‹ í˜¸
        """
        super().__init__()
        self.user_goal = user_goal
        self.task_queue = task_queue
        self.result_queue = result_queue
        self.status_event = status_event
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        if torch.backends.mps.is_available(): self.device = 'mps'

    def run(self):
        """í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì§„ì…ì """
        print(f"[Worker] ğŸš€ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (PID: {self.pid})")
        
        # ---------------------------------------------------------
        # Step A. ëª¨ë¸ ë¡œë“œ ë° ì´ˆê¸°í™” (Heavy Task - 1íšŒë§Œ ìˆ˜í–‰)
        # ---------------------------------------------------------
        print("[Worker] 1. ëª¨ë¸ ë¡œë”© ì¤‘...")
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ëª¨ë¸ ë¡œë“œ
        if USE_REAL_API:
            self.embed_model = SentenceTransformer(resource_path('./ai/emb'), device=self.device)
            self.genai_model = genai.GenerativeModel('gemini-2.5-flash')
        else:
            print("[Worker] (Mock ëª¨ë“œ) ëª¨ë¸ ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜")
            time.sleep(1) # ë¡œë”© ì‹œê°„ í‰ë‚´

        # ---------------------------------------------------------
        # Step B. ì¿¼ë¦¬ í™•ì¥ ë° ì‚¬ì „ ì„ë² ë”© (Pre-computation - 1íšŒë§Œ ìˆ˜í–‰)
        # ---------------------------------------------------------
        print(f"[Worker] 2. ëª©í‘œ í™•ì¥ ìˆ˜í–‰: '{self.user_goal}'")
        expanded_queries = self._expand_goal(self.user_goal)
        print(f"[Worker]    -> í™•ì¥ëœ ì¿¼ë¦¬ ëª©ë¡: {expanded_queries}")

        print("[Worker] 3. ì¿¼ë¦¬ ë²¡í„° ì‚¬ì „ ê³„ì‚° (Pre-encoding)...")
        # ì¿¼ë¦¬ ë²¡í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•´ì„œ ë©”ëª¨ë¦¬ì— ìƒì£¼ì‹œí‚´ (ì†ë„ í•µì‹¬)
        self.cached_query_embeddings = self._pre_encode_queries(expanded_queries)
        
        # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ê²Œ "ì¤€ë¹„ ì™„ë£Œ" ì‹ í˜¸ ë³´ëƒ„
        print("[Worker] âœ… ì¤€ë¹„ ì™„ë£Œ! ëŒ€ê¸° ì¤‘...")
        self.status_event.set()

        # ---------------------------------------------------------
        # Step C. ë¶„ì„ ë£¨í”„ (ë°˜ë³µ ìˆ˜í–‰)
        # ---------------------------------------------------------
        while True:
            try:
                # íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸° (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ì¤„ ë•Œê¹Œì§€ ëŒ€ê¸°)
                task = self.task_queue.get()
                
                # ì¢…ë£Œ ì‹ í˜¸ í™•ì¸
                if task == "STOP":
                    print("[Worker] ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ . í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

            except Exception as e:
                print(f"[Worker] ì—ëŸ¬ ë°œìƒ1: {e}")
                self.result_queue.put({"error": str(e)})   

            try:
                # ì›¹ í˜ì´ì§€ ë¶„ì„ ìˆ˜í–‰
                page_data = task
                start_t = time.time()
            except Exception as e:
                print(f"[Worker] ì—ëŸ¬ ë°œìƒ2-1: {e}")
                self.result_queue.put({"error": str(e)})  

            if(self._is_white(page_data['url'])):
                result = {
                    "is_focused": True,
                    "score": 1,
                    "matched_query": "WHITELIST",
                    "elapsed": 0
                }
                self.result_queue.put(result)
                continue
            if(self._is_black(page_data['url'])):
                result = {
                    "is_focused": False,
                    "score": 0,
                    "matched_query": "BLACKLIST",
                    "elapsed": 0
                }
                self.result_queue.put(result)
                continue

            try:
                score, maxidx = self._calculate_similarity(page_data)
                elapsed = time.time() - start_t

            except Exception as e:
                print(f"[Worker] ì—ëŸ¬ ë°œìƒ2-2: {e}")
                self.result_queue.put({"error": str(e)})      
            
            try:              
                # ê²°ê³¼ ì „ì†¡
                result = {
                    "is_focused": score >= 0.2394,
                    "score": score,
                    "matched_query": expanded_queries[maxidx] if score >= 0.2394 else "Distractive content",
                    "elapsed": elapsed
                }
                self.result_queue.put(result)
            except Exception as e:
                print(f"[Worker] ì—ëŸ¬ ë°œìƒ3: {e}")
                self.result_queue.put({"error": str(e)})

    # --- ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ ---

    def _expand_goal(self, goal):
        """
        ì‚¬ìš©ì ëª©í‘œë¥¼ ë°›ì•„ 3~4ê°œì˜ êµ¬ì²´ì ì¸ í•˜ìœ„ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        í•­ìƒ [ì›ë³¸ ì¿¼ë¦¬]ë¥¼ 0ë²ˆ ì¸ë±ìŠ¤ì— í¬í•¨ì‹œí‚´ (Anchor Query)
        """

        prompt = f"""
        Role: You are an expert in 'Semantic Network Analysis' and 'Knowledge Graph Construction'.

        Task: Deconstruct the User's Goal into 24 distinct "Semantic Anchors" to capture a wide range of relevant web content.
        An "Anchor" is a short, declarative statement (3-5 seconds reading time) representing content likely to be found on relevant web pages.

        User Goal: "{goal}"

        ***CRITICAL INSTRUCTION: LEXICAL DIVERSITY***
        Do NOT rely solely on the words present in the "User Goal". You must expand the vocabulary to include:
        1.  **Hierarchical Terms:** If the goal is "AI", you must include anchors about "Machine Learning", "Neural Networks", "Deep Learning", etc.
        2.  **Related Entities:** Specific libraries, tools, or famous authors related to the topic (e.g., "TensorFlow", "PyTorch", "Andrew Ng").
        3.  **Contextual Synonyms:** Words that naturally co-occur in the domain (e.g., for "Stock Analysis", use "Moving Average", "Candlestick Chart", "Volatility").

        Guidelines:
        1.  **Format:** Declarative, Factual, Descriptive phrases. (No Questions).
        2.  **Coverage:**
            - 8 Anchors: Broad/Conceptual definitions (High-level concepts).
            - 8 Anchors: Specific/Technical details (Sub-concepts, formulas, specific algorithms).
            - 8 Anchors: Practical/Tool-oriented context (Software, errors, implementation).
        3.  **Constraint:** Avoid repeating the exact main keywords of the User Goal in every anchor. Use pronouns or implied context to increase vector diversity.

        Output Format: JSON Array of strings ONLY. In english.
        """
        
        
        
        
        """
        
        Role: You are an expert in Web Content Classification.

        Task: Break down the User's Goal into 30 distinct "Semantic Anchors".
        An "Anchor" is NOT a search query (question). It is a short, declarative phrase or sentence that typically appears inside the target web pages.

        User Goal: "{goal}"

        Guidelines:
        1.  **Style:** Declarative, Factual, Descriptive. (Like a textbook heading or a Wikipedia summary sentence).
        2.  **Avoid:** Do not use question marks, "How to", "Help", or imperative verbs.
        3.  **Diversity:** Cover theoretical definitions, technical terminology, and practical application contexts.

        Example Idea (Goal: "Learn Python"):
        - Bad (Query): "How to install Python?"
        - Good (Anchor): "Step-by-step guide for installing Python environment on Windows and macOS."
        - Bad (Query): "Python list vs tuple"
        - Good (Anchor): "Differences between mutable Lists and immutable Tuples in Python data structures."

        Output Format: JSON Array of strings ONLY. In english.
           
        """
        
        
        
        """
        Role: You are an expert in 'Semantic Knowledge Representation' and Information Retrieval.

        Task: Deconstruct the User's Goal into 15 distinct semantic representations optimized for Vector Embedding Retrieval (RAG). 
        Instead of generating "search queries" (questions), generate "content descriptors" (phrases likely to appear in target documents).

        User Goal: "{goal}"

        Guidelines:
        1.  **Avoid Functional Noise:** Do not use words like "help", "assignment", "homework", "essay", or "solution" unless the goal is explicitly about finding a tutor. Focus on the *subject matter*.
        2.  **Diversity:** Generate phrases across three categories (5 queries each):
            * [Core Concepts]: Definitional and theoretical terminology (e.g., "Utility maximization logic").
            * [Contextual/Broad]: Related academic fields or real-world scenarios (e.g., "Market failure in public goods").
            * [Specific/Technical]: Deep technical terms, specific entities, or methodologies (e.g., "Lagrange multiplier method in economics").
        3.  **Format:** Generate a single JSON Array of strings containing all 15 phrases.

        Output Format: JSON Array of strings ONLY. No markdown. In english.
        """
        try:
            response = self.genai_model.generate_content(prompt)
            clean_text = response.text.replace("```json", "").replace("```", "").strip()
            expanded_list = json.loads(clean_text)
            
            # ì›ë³¸ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ë§¨ ì•ì— ì¶”ê°€ (Baseline ë³´ì¥)
            if goal not in expanded_list:
                expanded_list.insert(0, goal)
            return expanded_list
            
        except Exception as e:
            return [goal]

    def _preprocess(self, text):
        return " ".join(text.split())[:1000] if text else ""
    
    def _pre_encode_queries(self, queries):
        """ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (1íšŒ ìˆ˜í–‰)"""
        # Prefix ì¶”ê°€
        formatted_queries = [f"{self._preprocess(q)}" for q in queries]
        return self.embed_model.encode(formatted_queries, prompt_name='Retrieval-query')


    def _calculate_similarity(self, page_data):
        """ì›¹í˜ì´ì§€ ë²¡í„°í™” ë° ë¯¸ë¦¬ ê³„ì‚°ëœ ì¿¼ë¦¬ ë²¡í„°ì™€ ë¹„êµ"""
        title = self._preprocess(page_data.get('title', ''))
        meta = self._preprocess(page_data.get('meta', ''))
        body = self._preprocess(page_data.get('body', ''))
        
        doc_text = f"{meta}{body}"
        print("[EMBED]")
        print(f"TITLE:\t{title}\nMETA:\t{meta}\nBODY:\t{body[:300]}")
        # 1. ë¬¸ì„œë§Œ ì¸ì½”ë”© (ì¿¼ë¦¬ëŠ” ì´ë¯¸ self.cached_query_embeddingsì— ìˆìŒ)
        doc_emb = self.embed_model.encode(doc_text, prompt=f"title: {title} | text: ")
        
        # 2. í–‰ë ¬ ê³± (Query Batch x Document)
        # doc_embê°€ (768,) ì´ë©´ (1, 768)ë¡œ ë³€ê²½ í•„ìš”í•  ìˆ˜ ìˆìŒ
        scores = self.embed_model.similarity(self.cached_query_embeddings, doc_emb).numpy().flatten()
        
        # 3. Max Pooling
        max_idx = np.argmax(scores)
        best_score = float(scores[max_idx])
        return best_score, max_idx
    
    from urllib.parse import urlparse

    def _is_white(self, url):
        return check_list(url, WHITELIST)


    def _is_black(self, url):
        return check_list(url, BLACKLIST)

def normalize_url(url):
    if not url.startswith(('http://', 'https://')):
        url = 'http://' + url  # íŒŒì‹±ì„ ìœ„í•´ ì„ì‹œ ìŠ¤í‚¤ë§ˆ ì¶”ê°€
    
    parsed = urlparse(url)
    # netloc(ë„ë©”ì¸) + path(ê²½ë¡œ)ë¥¼ í•©ì¹˜ê³ , ëì˜ '/'ëŠ” ì œê±°í•˜ì—¬ í‘œì¤€í™”
    clean_url = (parsed.netloc + parsed.path).rstrip('/')
    return parsed.netloc, clean_url

def check_list(input_url, list_data):

    input_domain, input_full_clean = normalize_url(input_url)

    for item in list_data:
        target_url = item['url']
        collective= item['collect']
        
        target_domain, target_full_clean = normalize_url(target_url)

        if collective:
            if input_domain == target_domain:
                return True
        else:
            if input_full_clean == target_full_clean:
                return True
                
    return False
